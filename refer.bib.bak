% Encoding: UTF-8
@article{Colton2012Full,
  title={Full-FACE Poetry Generation},
  author={Colton, Simon and Goodwin, Jacob and Veale, Tony},
  year={2012},
}

@article{Oliveira2015Tra,
  title={Tra-la-Lyrics 2.0: Automatic Generation of Song Lyrics on a Semantic Domain},
  author={Oliveira, Hugo Gon√ßalo},
  journal={Journal of Artificial General Intelligence},
  volume={6},
  number={1},
  pages={87-110},
  year={2015},
}

@inproceedings{Ghazvininejad2016Generating,
  title={Generating Topical Poetry},
  author={Ghazvininejad, Marjan and Shi, Xing and Choi, Yejin and Knight, Kevin},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={1183-1191},
  year={2016},
}

@article{Yi2017Generating,
  title={Generating Chinese Classical Poems with RNN Encoder-Decoder},
  author={Yi, Xiaoyuan and Li, Ruoyu and Sun, Maosong},
  year={2017},
}

@inproceedings{Zhang2014Chinese,
  title={Chinese Poetry Generation with Recurrent Neural Networks},
  author={Zhang, Xingxing and Lapata, Mirella},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={670-680},
  year={2014},
}

@article{Kalchbrenner2016Neural,
  title={Neural Machine Translation in Linear Time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron Van Den and Graves, Alex and Kavukcuoglu, Koray},
  year={2016},
}

@article{Zhou2016Deep,
  title={Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation},
  author={Zhou, Jie and Cao, Ying and Wang, Xuguang and Li, Peng and Xu, Wei},
  year={2016},
}

@article{Wu2016Google,
  title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
  year={2016},
}
 
 @article{gedikli2014should,  
title={How should I explain? A comparison of different explanation types for recommender systems},  
author={Gedikli, Fatih and Jannach, Dietmar and Ge, Mouzhi},  
journal={International Journal of Human-Computer Studies},  
volume={72},  
number={4},  
pages={367--382},  
year={2014},  
publisher={Elsevier}  
}  
 
@inproceedings{wu2015flame,  
 title={Flame: A probabilistic model combining aspect based opinion mining and collaborative filtering},  
 author={Wu, Yao and Ester, Martin},  
 booktitle={Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},  
pages={199--208},  
year={2015},  
organization={ACM}  
}  
  
 @inproceedings{herlocker2000explaining,  
title={Explaining collaborative filtering recommendations},  
 author={Herlocker, Jonathan L and Konstan, Joseph A and Riedl, John},  
 booktitle={Proceedings of the 2000 ACM conference on Computer supported cooperative work},  
pages={241--250},  
year={2000},  
organization={ACM}  
}  
   
@article{zhang2017entity,  
title={Entity suggestion with conceptual explanation},  
author={Zhang, Yi and Xiao, Yanghua and Hwang, Seung-won and Wang, Haixun and Wang, X Sean and Wang, Wei},  
journal={IJCAI2017},  
year={2017}  
}  
  
@inproceedings{ozbal2013brainsup,  
title={BRAINSUP: Brainstorming support for creative sentence generation},  
 author={{\"O}zbal, G{\"o}zde and Pighin, Daniele and Strapparava, Carlo},  
 booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},  
volume={1},  
pages={1446--1455},  
year={2013}  
}  


@article{ratner2017snorkel,
  title={Snorkel: Rapid training data creation with weak supervision},
  author={Ratner, Alexander and Bach, Stephen H and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={3},
  pages={269--282},
  year={2017},
  publisher={VLDB Endowment}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@article{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  journal={Text Summarization Branches Out},
  year={2004}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{wang2016chinese,
  title={Chinese poetry generation with planning based neural network},
  author={Wang, Zhe and He, Wei and Wu, Hua and Wu, Haiyang and Li, Wei and Wang, Haifeng and Chen, Enhong},
  journal={arXiv preprint arXiv:1610.09889},
  year={2016}
}

@inproceedings{munigala2018persuaide,
  title={PersuAIDE! An Adaptive Persuasive Text Generation System for Fashion Domain},
  author={Munigala, Vitobha and Mishra, Abhijit and Tamilselvam, Srikanth G and Khare, Shreya and Dasgupta, Riddhiman and Sankaran, Anush},
  booktitle={Companion of the The Web Conference 2018 on The Web Conference 2018},
  pages={335--342},
  year={2018},
  organization={International World Wide Web Conferences Steering Committee}
}

@article{gu2016incorporating,
  title={Incorporating copying mechanism in sequence-to-sequence learning},
  author={Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor OK},
  journal={arXiv preprint arXiv:1603.06393},
  year={2016}
}

@InProceedings{Zhang2015Daily,
  author    = {Zhang, Yongfeng and Zhang, Min and Zhang, Yi and Lai, Guokun and Liu, Yiqun and Zhang, Honghui and Ma, Shaoping},
  title     = {Daily-Aware Personalized Recommendation Based on Feature-Level Time Series Analysis},
  booktitle = {Proceedings of the 24th International Conference on World Wide Web},
  year      = {2015},
  series    = {WWW '15},
  pages     = {1373--1383},
  address   = {Republic and Canton of Geneva, Switzerland},
  publisher = {International World Wide Web Conferences Steering Committee},
  abstract  = {The frequently changing user preferences and/or item profiles have put essential importance on the dynamic modeling of users and items in personalized recommender systems. However, due to the insufficiency of per user/item records when splitting the already sparse data across time dimension, previous methods have to restrict the drifting purchasing patterns to pre-assumed distributions, and were hardly able to model them rather directly with, for example, time series analysis. Integrating content information helps to alleviate the problem in practical systems, but the domain-dependent content knowledge is expensive to obtain due to the large amount of manual efforts.

In this paper, we make use of the large volume of textual reviews for the automatic extraction of domain knowledge, namely, the explicit features/aspects in a specific product domain. We thus degrade the product-level modeling of user preferences, which suffers from the lack of data, to the feature-level modeling, which not only grants us the ability to predict user preferences through direct time series analysis, but also allows us to know the essence under the surface of product-level changes in purchasing patterns. Besides, the expanded feature space also helps to make cold-start recommendations for users with few purchasing records.

Technically, we develop the Fourier-assisted Auto-Regressive Integrated Moving Average (FARIMA) process to tackle with the year-long seasonal period of purchasing data to achieve daily-aware preference predictions, and we leverage the conditional opportunity models for daily-aware personalized recommendation. Extensive experimental results on real-world cosmetic purchasing data from a major e-commerce website (JD.com) in China verified both the effectiveness and efficiency of our approach.
},
  acmid     = {2741087},
  doi       = {10.1145/2736277.2741087},
  isbn      = {978-1-4503-3469-3},
  keywords  = {collaborative filtering, recommender systems, sentiment analysis, time series analysis},
  location  = {Florence, Italy},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2736277.2741087},
}

@InProceedings{Chen2018Neural,
  author    = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
  title     = {Neural Attentional Rating Regression with Review-level Explanations},
  booktitle = {Proceedings of the 2018 World Wide Web Conference},
  year      = {2018},
  series    = {WWW '18},
  pages     = {1583--1592},
  address   = {Republic and Canton of Geneva, Switzerland},
  publisher = {International World Wide Web Conferences Steering Committee},
  abstract  = {Reviews information is dominant for users to make online purchasing decisions in e-commerces. However, the usefulness of reviews is varied. We argue that less-useful reviews hurt model's performance, and are also less meaningful for user's reference. While some existing models utilize reviews for improving the performance of recommender systems, few of them consider the usefulness of reviews for recommendation quality. In this paper, we introduce a novel attention mechanism to explore the usefulness of reviews, and propose a Neural Attentional Regression model with Review-level Explanations (NARRE) for recommendation. Specifically, NARRE can not only predict precise ratings, but also learn the usefulness of each review simultaneously. Therefore, the highly-useful reviews are obtained which provide review-level explanations to help users make better and faster decisions. Extensive experiments on benchmark datasets of Amazon and Yelp on different domains show that the proposed NARRE model consistently outperforms the state-of-the-art recommendation approaches, including PMF, NMF, SVD++, HFT, and DeepCoNN in terms of rating prediction, by the proposed attention model that takes review usefulness into consideration. Furthermore, the selected reviews are shown to be effective when taking existing review-usefulness ratings in the system as ground truth. Besides, crowd-sourcing based evaluations reveal that in most cases, NARRE achieves equal or even better performances than system's usefulness rating method in selecting reviews. And it is flexible to offer great help on the dominant cases in real e-commerce scenarios when the ratings on review-usefulness are not available in the system.},
  acmid     = {3186070},
  comment   = {contribution: a Neural Attentional
Regression model with Review-level Explanations (NARRE)

the attention mechanism to automatically assign
weights to reviews in a distant supervised manner

review-level explanation, providing users with the highly-useful
reviews},
  doi       = {10.1145/3178876.3186070},
  isbn      = {978-1-4503-5639-8},
  keywords  = {explainable recommendation, neural attention network, recommender systems, review usefulness},
  location  = {Lyon, France},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3178876.3186070},
}

@InBook{Yoo2011Creating,
  pages     = {455--477},
  title     = {Creating More Credible and Persuasive Recommender Systems: The Influence of Source Characteristics on Recommender System Evaluations},
  publisher = {Springer US},
  year      = {2011},
  author    = {Yoo, Kyung-Hyan and Gretzel, Ulrike},
  editor    = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  address   = {Boston, MA},
  isbn      = {978-0-387-85820-3},
  abstract  = {Whether users are likely to accept the recommendations provided by a recommender system is of utmost importance to system designers and the marketers who implement them. By conceptualizing the advice seeking and giving relationship as a fundamentally social process, important avenues for understanding the persuasiveness of recommender systems open up. Specifically, research regarding the influence of source characteristics, which is abundant in the context of humanhuman relationships, can provide an important framework for identifying potential influence factors. This chapter reviews the existing literature on source characteristics in the context of human-human, human-computer, and human-recommender system interactions. It concludes that many social cues that have been identified as influential in other contexts have yet to be implemented and tested with respect to recommender systems. Implications for recommender system research and design are discussed.},
  booktitle = {Recommender Systems Handbook},
  comment   = {persuasiveness related to trust },
  doi       = {10.1007/978-0-387-85820-3_14},
  url       = {https://doi.org/10.1007/978-0-387-85820-3_14},
}

@InBook{Tintarev2011Designing,
  pages     = {479--510},
  title     = {Designing and Evaluating Explanations for Recommender Systems},
  publisher = {Springer US},
  year      = {2011},
  author    = {Tintarev, Nava and Masthoff, Judith},
  editor    = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  address   = {Boston, MA},
  isbn      = {978-0-387-85820-3},
  abstract  = {This chapter gives an overview of the area of explanations in recommender systems. We approach the literature from the angle of evaluation: that is, we are interested in what makes an explanation ``good'', and suggest guidelines as how to best evaluate this. We identify seven benefits that explanations may contribute to a recommender system, and relate them to criteria used in evaluations of explanations in existing systems, and how these relate to evaluations with live recommender systems. We also discuss how explanations can be affected by how recommendations are presented, and the role the interaction with the recommender system plays w.r.t. explanations. Finally, we describe a number of explanation styles, and how they may be related to the underlying algorithms. Examples of explanations in existing systems are mentioned throughout.},
  booktitle = {Recommender Systems Handbook},
  comment   = {goals: 
transparency
scrutability
trust
effectiveness
persuasiveness
efficiency
satisfaction},
  doi       = {10.1007/978-0-387-85820-3_15},
  url       = {https://doi.org/10.1007/978-0-387-85820-3_15},
}

@InProceedings{Costa2018Automatic,
  author    = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
  title     = {Automatic Generation of Natural Language Explanations},
  booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
  year      = {2018},
  series    = {IUI '18 Companion},
  pages     = {57:1--57:2},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3180366},
  articleno = {57},
  doi       = {10.1145/3180308.3180366},
  isbn      = {978-1-4503-5571-1},
  keywords  = {Explainability, Explanations, Natural Language Generation, Neural Network, Recommender systems},
  location  = {Tokyo, Japan},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/3180308.3180366},
}

@Article{Bengio2003neural,
  author    = {Yoshua Bengio and Rejean Ducharme and Pascal Vincent and Christian Jauvin},
  title     = {A neural probabilistic language model},
  journal   = {Journal of Machine Learning Research},
  year      = {2003},
  volume    = {3},
  number    = {Feb},
  pages     = {1137-1155},
  booktitle = {The Thirty-second Annual Conference on Neural Information Processing Systems},
}

@InProceedings{Hua2018Shakespeare,
  author    = {Qingsong Hua},
  title     = {Shakespeare of Alibaba: Practice of Intelligent Recommendation Reason Generation in Alibaba},
  booktitle = {Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2018},
}

@InProceedings{Wiseman2017Challenges,
  author    = {Wiseman, Sam and Shieber, Stuart and Rush, Alexander},
  title     = {Challenges in Data-to-Document Generation},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year      = {2017},
  pages     = {2253--2263},
  abstract  = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
  timestamp = {2019-01-18},
}

@InProceedings{Yang2017Reference,
  author    = {Yang, Zichao and Blunsom, Phil and Dyer, Chris and Ling, Wang},
  title     = {Reference-Aware Language Models},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year      = {2017},
  pages     = {1850--1859},
  abstract  = {We propose a general class of language
models that treat reference as discrete
stochastic latent variables. This decision
allows for the creation of entity mentions
by accessing external databases of referents
(required by, e.g., dialogue generation)
or past internal state (required to explicitly
model coreferentiality). Beyond
simple copying, our coreference model
can additionally refer to a referent using
varied mention forms (e.g., a reference to
‚ÄúJane‚Äù can be realized as ‚Äúshe‚Äù), a characteristic
feature of reference in natural languages.
Experiments on three representative
applications show our model variants
outperform models based on deterministic
attention and standard language modeling
baselines.},
  comment   = { enable
coreferent mentions to have different forms},
}

@InProceedings{Lebret2016Neural,
  author    = {Lebret, R{\'e}mi and Grangier, David and Auli, Michael},
  title     = {Neural Text Generation from Structured Data with Application to the Biography Domain},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {1203--1213},
  abstract  = {This paper introduces a neural model for
concept-to-text generation that scales to large,
rich domains. It generates biographical sentences
from fact tables on a new dataset of
biographies from Wikipedia. This set is an
order of magnitude larger than existing resources
with over 700k samples and a 400k
vocabulary. Our model builds on conditional
neural language models for text generation.
To deal with the large vocabulary, we extend
these models to mix a fixed vocabulary
with copy actions that transfer sample-specific
words from the input database to the generated
output sentence. To deal with structured
data, we allow the model to embed words
differently depending on the data fields in
which they occur. Our neural model significantly
outperforms a Templated Kneser-Ney
language model by nearly 15 BLEU},
  comment   = {exploits structured data both globally
and locally

word embedding, table embedding (global + local), aggregating

feed-forward neural language model},
}

@Article{Bao2019Text,
  author    = {J. Bao and D. Tang and N. Duan and Z. Yan and M. Zhou and T. Zhao},
  title     = {Text Generation From Tables},
  journal   = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year      = {2019},
  volume    = {27},
  number    = {2},
  pages     = {311-320},
  month     = {Feb},
  issn      = {2329-9290},
  abstract  = {This paper proposes a neural generative model, namely Table2Seq, to generate a natural language sentence based on a table. Specifically, the model maps a table to continuous vectors and then generates a natural language sentence by leveraging the semantics of a table. Since rare words, e.g., entities and values, usually appear in a table, we develop a flexible copying mechanism that selectively replicates contents from the table to the output sequence. We conduct extensive experiments to demonstrate the effectiveness of our Table2Seq model and the utility of the designed copying mechanism. On the WIKIBIO and SIMPLEQUESTIONS datasets, the Table2Seq model improves the state-of-the-art results from 34.70 to 40.26 and from 33.32 to 39.12 in terms of BLEU-4 scores, respectively. Moreover, we construct an open-domain dataset WIKITABLETEXT that includes 13 318 descriptive sentences for 4962 tables. Our Table2Seq model achieves a BLEU-4 score of 38.23 on WIKITABLETEXT outperforming template-based and language model based approaches. Furthermore, through experiments on 1 M table-query pairs from a search engine, our Table2Seq model considering the structured part of a table, i.e., table attributes and table cells, as additional information outperforms a sequence-to-sequence model considering only the sequential part of a table, i.e., table caption.},
  comment   = {encoder: just embeddig?
decoder: rnn
global-local
copy a new copy model?
},
  doi       = {10.1109/TASLP.2018.2878381},
  keywords  = {natural language processing;neural nets;text analysis;text generation;continuous vectors;table semantics;flexible copying mechanism;selective content replication;WIKIBIO dataset;SIMPLEQUESTIONS dataset;BLEU-4 scores;WIKITABLETEXT dataset;descriptive sentences;search engine;table-query pairs;language model based approaches;natural language sentence;neural generative model;table caption;table cells;table attributes;Table2Seq model;Natural languages;Task analysis;Search engines;Neural networks;Speech processing;Decoding;Encyclopedias;Table-to-text generation;Table2Seq;neural network;natural language processing;artificial intelligence},
  timestamp = {2019-01-18},
}

@InProceedings{Thomaidou2013Automated,
  author    = {Thomaidou, Stamatina and Lourentzou, Ismini and Katsivelis-Perakis, Panagiotis and Vazirgiannis, Michalis},
  title     = {Automated snippet generation for online advertising},
  booktitle = {Proceedings of the 22nd ACM international conference on Conference on information \&\#38; knowledge management},
  year      = {2013},
  series    = {CIKM '13},
  pages     = {1841--1844},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Products, services or brands can be advertised alongside the search results in major search engines, while recently smaller displays on devices like tablets and smartphones have imposed the need for smaller ad texts. In this paper, we propose a method that produces in an automated manner compact text ads (promotional text snippets), given as input a product description webpage (landing page). The challenge is to produce a small comprehensive ad while maintaining at the same time relevance, clarity, and attractiveness. Our method includes the following phases. Initially, it extracts relevant and important n-grams (keywords) given the landing page. The keywords reserved must have a positive meaning in order to have a call-to-action style, thus we attempt sentiment analysis on them. Next, we build an Advertising Language Model to evaluate phrases in terms of their marketing appeal. We experiment with two variations of our method and we show that they outperform all the baseline approaches.},
  acmid     = {2507876},
  doi       = {10.1145/2505515.2507876},
  isbn      = {978-1-4503-2263-8},
  keywords  = {automated ad-text generation, online advertising, sponsored search, textual advertising},
  location  = {San Francisco, California, USA},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/2505515.2507876},
}

@Article{Kukich1983Knowledge,
  author     = {Kukich, Karen},
  title      = {Knowledge-Based Report Generation: A Technique for Automatically Generating Natural Language Reports from Databases},
  journal    = {SIGIR Forum},
  year       = {1983},
  volume     = {17},
  number     = {4},
  pages      = {246--250},
  month      = jun,
  issn       = {0163-5840},
  abstract   = {Knowledge-Based Report Generation is a technique for automatically generating natural language summaries from databases. It is so named because it applies the tools of knowledge-based expert systems design to the problem of text generation. The technique is currently being applied to the design of an automatic natural language stock report generator. Examples drawn from the implementation of the stock report generator are used to describe the components of a knowledge-based report generator.},
  acmid      = {511828},
  address    = {New York, NY, USA},
  doi        = {10.1145/1013230.511828},
  issue_date = {Summer 1983},
  keywords   = {databases, knowledge-based expert systems, natural language processing, text generation},
  numpages   = {5},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1013230.511828},
}

@InProceedings{Sutskever2011Generating,
  author    = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  title     = {Generating text with recurrent neural networks},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  year      = {2011},
  pages     = {1017--1024},
  abstract  = {Recurrent Neural Networks (RNNs) are very
powerful sequence models that do not enjoy
widespread use because it is extremely diffi-
cult to train them properly. Fortunately, recent
advances in Hessian-free optimization have
been able to overcome the difficulties associated
with training RNNs, making it possible to apply
them successfully to challenging sequence problems.
In this paper we demonstrate the power
of RNNs trained with the new Hessian-Free optimizer
(HF) by applying them to character-level
language modeling tasks. The standard RNN architecture,
while effective, is not ideally suited
for such tasks, so we introduce a new RNN
variant that uses multiplicative (or ‚Äúgated‚Äù) connections
which allow the current input character
to determine the transition matrix from one
hidden state vector to the next. After training
the multiplicative RNN with the HF optimizer
for five days on 8 high-end Graphics Processing
Units, we were able to surpass the performance
of the best previous single method for characterlevel
language modeling ‚Äì a hierarchical nonparametric
sequence model. To our knowledge
this represents the largest recurrent neural network
application to date.},
}

@InProceedings{Li2018Point,
  author    = {Li, Liunian and Wan, Xiaojun},
  title     = {Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  year      = {2018},
  pages     = {1044--1055},
  abstract  = {The task of data-to-text generation aims to generate descriptive texts conditioned on a number
of database records, and recent neural models have shown significant progress on this task. The
attention based encoder-decoder models with copy mechanism have achieved state-of-the-art
results on a few data-to-text datasets. However, such models still face the problem of putting
incorrect data records in the generated texts, especially on some more challenging datasets like
ROTOWIRE. In this paper, we propose a two-stage approach with a delayed copy mechanism
to improve the precision of data records in the generated texts. Our approach first adopts an
encoder-decoder model to generate a template text with data slots to be filled and then leverages
a proposed delayed copy mechanism to fill in the slots with proper data records. Our delayed
copy mechanism can take into account all the information of the input data records and the
full generated template text by using double attention, position-aware attention and a pairwise
ranking loss. The two models in the two stages are trained separately. Evaluation results on the
ROTOWIRE dataset verify the efficacy of our proposed approach to generate better templates and
copy data records more precisely.},
}

@InProceedings{Marcheggiani2018Deep,
  author    = {Marcheggiani, Diego and Perez-Beltrachini, Laura},
  title     = {Deep Graph Convolutional Encoders for Structured Data to Text Generation},
  booktitle = {Proceedings of the 11th International Conference on Natural Language Generation},
  year      = {2018},
  pages     = {1--9},
}

@InProceedings{Karpathy2015Deep,
  author    = {Karpathy, Andrej and Fei-Fei, Li},
  title     = {Deep visual-semantic alignments for generating image descriptions},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year      = {2015},
  pages     = {3128--3137},
}

@InProceedings{Dehghani2017Neural,
  author    = {Dehghani, Mostafa and Zamani, Hamed and Severyn, Aliaksei and Kamps, Jaap and Croft, W. Bruce},
  title     = {Neural Ranking Models with Weak Supervision},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2017},
  series    = {SIGIR '17},
  pages     = {65--74},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet effective ranking models based on feed-forward neural networks. We study their effectiveness under various learning scenarios (point-wise and pair-wise models) and using different input representations (i.e., from encoding query-document pairs into dense/sparse vectors to using word embedding representation). We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection (Robust) and a heterogeneous large-scale web collection (ClueWeb). Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections. Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.},
  acmid     = {3080832},
  doi       = {10.1145/3077136.3080832},
  isbn      = {978-1-4503-5022-8},
  keywords  = {ad-hoc retrieval, deep learning, deep neural network, ranking model, weak supervision},
  location  = {Shinjuku, Tokyo, Japan},
  numpages  = {10},
  timestamp = {2019-01-10},
  url       = {http://doi.acm.org/10.1145/3077136.3080832},
}

@Comment{jabref-meta: databaseType:bibtex;}
