%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for cheyenne at 2019-01-22 21:25:36 +0800 


%% Saved with string encoding Unicode (UTF-8) 


@comment{jabref-meta: databaseType:bibtex;}



@article{Oliveira2015Tra,
	Author = {Oliveira, Hugo Gon{\c c}alo},
	Journal = {Journal of Artificial General Intelligence},
	Number = {1},
	Pages = {87-110},
	Title = {Tra-la-Lyrics 2.0: Automatic Generation of Song Lyrics on a Semantic Domain},
	Volume = {6},
	Year = {2015}}

@inproceedings{Ghazvininejad2016Generating,
	Author = {Ghazvininejad, Marjan and Shi, Xing and Choi, Yejin and Knight, Kevin},
	Booktitle = {Conference on Empirical Methods in Natural Language Processing},
	Pages = {1183-1191},
	Title = {Generating Topical Poetry},
	Year = {2016}}

@inproceedings{Zhang2014Chinese,
	Author = {Zhang, Xingxing and Lapata, Mirella},
	Booktitle = {Conference on Empirical Methods in Natural Language Processing},
	Pages = {670-680},
	Title = {Chinese Poetry Generation with Recurrent Neural Networks},
	Year = {2014}}

@article{gedikli2014should,
	Author = {Gedikli, Fatih and Jannach, Dietmar and Ge, Mouzhi},
	Journal = {International Journal of Human-Computer Studies},
	Number = {4},
	Pages = {367--382},
	Publisher = {Elsevier},
	Title = {How should I explain? A comparison of different explanation types for recommender systems},
	Volume = {72},
	Year = {2014}}

@inproceedings{tomavsic2014implementation,
	Author = {Tomavsic, Polona and Znidarvsic, M and Papa, Gregor},
	Booktitle = {Proceedings of 5th International Conference on Computational Creativity, Ljubljana, Slovenia},
	Number = {2014},
	Pages = {340--343},
	Title = {Implementation of a slogan generator},
	Volume = {301},
	Year = {2014}}

@inproceedings{wu2015flame,
	Author = {Wu, Yao and Ester, Martin},
	Booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
	Organization = {ACM},
	Pages = {199--208},
	Title = {Flame: A probabilistic model combining aspect based opinion mining and collaborative filtering},
	Year = {2015}}

@inproceedings{herlocker2000explaining,
	Author = {Herlocker, Jonathan L and Konstan, Joseph A and Riedl, John},
	Booktitle = {Proceedings of the 2000 ACM conference on Computer supported cooperative work},
	Organization = {ACM},
	Pages = {241--250},
	Title = {Explaining collaborative filtering recommendations},
	Year = {2000}}

@article{zhang2017entity,
	Author = {Zhang, Yi and Xiao, Yanghua and Hwang, Seung-won and Wang, Haixun and Wang, X Sean and Wang, Wei},
	Journal = {IJCAI2017},
	Title = {Entity suggestion with conceptual explanation},
	Year = {2017}}

@inproceedings{ozbal2013brainsup,
	Author = {Ozbal, Gozde and Pighin, Daniele and Strapparava, Carlo},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Pages = {1446--1455},
	Title = {BRAINSUP: Brainstorming support for creative sentence generation},
	Volume = {1},
	Year = {2013}}

@article{ratner2017snorkel,
	Author = {Ratner, Alexander and Bach, Stephen H and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
	Journal = {Proceedings of the VLDB Endowment},
	Number = {3},
	Pages = {269--282},
	Publisher = {VLDB Endowment},
	Title = {Snorkel: Rapid training data creation with weak supervision},
	Volume = {11},
	Year = {2017}}

@inproceedings{vaswani2017attention,
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {5998--6008},
	Title = {Attention is all you need},
	Year = {2017}}

@article{lin2004rouge,
	Author = {Lin, Chin-Yew},
	Journal = {Text Summarization Branches Out},
	Title = {Rouge: A package for automatic evaluation of summaries},
	Year = {2004}}

@inproceedings{papineni2002bleu,
	Author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	Booktitle = {Proceedings of the 40th annual meeting on association for computational linguistics},
	Organization = {Association for Computational Linguistics},
	Pages = {311--318},
	Title = {BLEU: a method for automatic evaluation of machine translation},
	Year = {2002}}

@inproceedings{wang2016chinese,
	Author = {Wang, Zhe and He, Wei and Wu, Hua and Wu, Haiyang and Li, Wei and Wang, Haifeng and Chen, Enhong},
	Booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
	Pages = {1051--1060},
	Title = {Chinese Poetry Generation with Planning based Neural Network},
	Year = {2016}}

@inproceedings{munigala2018persuaide,
	Author = {Munigala, Vitobha and Mishra, Abhijit and Tamilselvam, Srikanth G and Khare, Shreya and Dasgupta, Riddhiman and Sankaran, Anush},
	Booktitle = {Companion of the The Web Conference 2018 on The Web Conference 2018},
	Organization = {International World Wide Web Conferences Steering Committee},
	Pages = {335--342},
	Title = {PersuAIDE! An Adaptive Persuasive Text Generation System for Fashion Domain},
	Year = {2018}}

@article{gu2016incorporating,
	Author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor OK},
	Date-Modified = {2019-01-22 12:43:24 +0000},
	Journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
	Pages = {1631-1640},
	Title = {Incorporating copying mechanism in sequence-to-sequence learning},
	Year = {2016}}

@inproceedings{kiddon2011double,
	Author = {Kiddon, Chloe and Brun, Yuriy},
	Booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2},
	Organization = {Association for Computational Linguistics},
	Pages = {89--94},
	Title = {That's what she said: double entendre identification},
	Year = {2011}}

@inproceedings{valitutti2013let,
	Author = {Valitutti, Alessandro and Toivonen, Hannu and Doucet, Antoine and Toivanen, Jukka M},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	Pages = {243--248},
	Title = {"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints},
	Volume = {2},
	Year = {2013}}

@inproceedings{Zhang2015Daily,
	Abstract = {The frequently changing user preferences and/or item profiles have put essential importance on the dynamic modeling of users and items in personalized recommender systems. However, due to the insufficiency of per user/item records when splitting the already sparse data across time dimension, previous methods have to restrict the drifting purchasing patterns to pre-assumed distributions, and were hardly able to model them rather directly with, for example, time series analysis. Integrating content information helps to alleviate the problem in practical systems, but the domain-dependent content knowledge is expensive to obtain due to the large amount of manual efforts.

In this paper, we make use of the large volume of textual reviews for the automatic extraction of domain knowledge, namely, the explicit features/aspects in a specific product domain. We thus degrade the product-level modeling of user preferences, which suffers from the lack of data, to the feature-level modeling, which not only grants us the ability to predict user preferences through direct time series analysis, but also allows us to know the essence under the surface of product-level changes in purchasing patterns. Besides, the expanded feature space also helps to make cold-start recommendations for users with few purchasing records.

Technically, we develop the Fourier-assisted Auto-Regressive Integrated Moving Average (FARIMA) process to tackle with the year-long seasonal period of purchasing data to achieve daily-aware preference predictions, and we leverage the conditional opportunity models for daily-aware personalized recommendation. Extensive experimental results on real-world cosmetic purchasing data from a major e-commerce website (JD.com) in China verified both the effectiveness and efficiency of our approach.
},
	Acmid = {2741087},
	Address = {Republic and Canton of Geneva, Switzerland},
	Author = {Zhang, Yongfeng and Zhang, Min and Zhang, Yi and Lai, Guokun and Liu, Yiqun and Zhang, Honghui and Ma, Shaoping},
	Booktitle = {Proceedings of the 24th International Conference on World Wide Web},
	Doi = {10.1145/2736277.2741087},
	Isbn = {978-1-4503-3469-3},
	Keywords = {collaborative filtering, recommender systems, sentiment analysis, time series analysis},
	Location = {Florence, Italy},
	Numpages = {11},
	Pages = {1373--1383},
	Publisher = {International World Wide Web Conferences Steering Committee},
	Series = {WWW '15},
	Title = {Daily-Aware Personalized Recommendation Based on Feature-Level Time Series Analysis},
	Url = {https://doi.org/10.1145/2736277.2741087},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1145/2736277.2741087},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2736277.2741087}}

@inproceedings{Chen2018Neural,
	Address = {Republic and Canton of Geneva, Switzerland},
	Author = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
	Booktitle = {Proceedings of the 2018 World Wide Web Conference},
	Date-Modified = {2019-01-22 12:39:21 +0000},
	Numpages = {10},
	Pages = {1583--1592},
	Publisher = {International World Wide Web Conferences Steering Committee},
	Series = {WWW '18},
	Title = {Neural Attentional Rating Regression with Review-level Explanations},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1145/3178876.3186070},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3178876.3186070}}

@inbook{Yoo2011Creating,
	Abstract = {Whether users are likely to accept the recommendations provided by a recommender system is of utmost importance to system designers and the marketers who implement them. By conceptualizing the advice seeking and giving relationship as a fundamentally social process, important avenues for understanding the persuasiveness of recommender systems open up. Specifically, research regarding the influence of source characteristics, which is abundant in the context of humanhuman relationships, can provide an important framework for identifying potential influence factors. This chapter reviews the existing literature on source characteristics in the context of human-human, human-computer, and human-recommender system interactions. It concludes that many social cues that have been identified as influential in other contexts have yet to be implemented and tested with respect to recommender systems. Implications for recommender system research and design are discussed.},
	Address = {Boston, MA},
	Author = {Yoo, Kyung-Hyan and Gretzel, Ulrike},
	Booktitle = {Recommender Systems Handbook},
	Comment = {persuasiveness related to trust},
	Doi = {10.1007/978-0-387-85820-3_14},
	Editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
	Isbn = {978-0-387-85820-3},
	Pages = {455--477},
	Publisher = {Springer US},
	Title = {Creating More Credible and Persuasive Recommender Systems: The Influence of Source Characteristics on Recommender System Evaluations},
	Url = {https://doi.org/10.1007/978-0-387-85820-3_14},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-0-387-85820-3_14},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-0-387-85820-3_14}}

@inbook{Tintarev2011Designing,
	Abstract = {This chapter gives an overview of the area of explanations in recommender systems. We approach the literature from the angle of evaluation: that is, we are interested in what makes an explanation ``good'', and suggest guidelines as how to best evaluate this. We identify seven benefits that explanations may contribute to a recommender system, and relate them to criteria used in evaluations of explanations in existing systems, and how these relate to evaluations with live recommender systems. We also discuss how explanations can be affected by how recommendations are presented, and the role the interaction with the recommender system plays w.r.t. explanations. Finally, we describe a number of explanation styles, and how they may be related to the underlying algorithms. Examples of explanations in existing systems are mentioned throughout.},
	Address = {Boston, MA},
	Author = {Tintarev, Nava and Masthoff, Judith},
	Booktitle = {Recommender Systems Handbook},
	Comment = {goals: transparency scrutability trust effectiveness persuasiveness efficiency satisfaction},
	Doi = {10.1007/978-0-387-85820-3_15},
	Editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
	Isbn = {978-0-387-85820-3},
	Pages = {479--510},
	Publisher = {Springer US},
	Title = {Designing and Evaluating Explanations for Recommender Systems},
	Url = {https://doi.org/10.1007/978-0-387-85820-3_15},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-0-387-85820-3_15},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-0-387-85820-3_15}}

@inproceedings{Costa2018Automatic,
	Address = {New York, NY, USA},
	Author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
	Booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
	Date-Modified = {2019-01-22 12:39:47 +0000},
	Numpages = {2},
	Pages = {57:1--57:2},
	Publisher = {ACM},
	Title = {Automatic Generation of Natural Language Explanations},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3180308.3180366},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3180308.3180366}}

@article{Bengio2003neural,
	Author = {Y. Bengio and R. Ducharme and P. Vincent and C. Jauvin},
	Booktitle = {The Thirty-second Annual Conference on Neural Information Processing Systems},
	Date-Modified = {2019-01-22 12:38:53 +0000},
	Journal = {Journal of Machine Learning Research},
	Number = {Feb},
	Pages = {1137-1155},
	Title = {A neural probabilistic language model},
	Volume = {3},
	Year = {2003}}

@inproceedings{Hua2018Shakespeare,
	Author = {Qingsong Hua},
	Booktitle = {Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval},
	Title = {Shakespeare of Alibaba: Practice of Intelligent Recommendation Reason Generation in Alibaba},
	Year = {2018}}

@inproceedings{Wiseman2017Challenges,
	Abstract = {Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.},
	Author = {Wiseman, Sam and Shieber, Stuart and Rush, Alexander},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Pages = {2253--2263},
	Timestamp = {2019-01-18},
	Title = {Challenges in Data-to-Document Generation},
	Year = {2017}}

@inproceedings{Yang2017Reference,
	Abstract = {We propose a general class of language
models that treat reference as discrete
stochastic latent variables. This decision
allows for the creation of entity mentions
by accessing external databases of referents
(required by, e.g., dialogue generation)
or past internal state (required to explicitly
model coreferentiality). Beyond
simple copying, our coreference model
can additionally refer to a referent using
varied mention forms (e.g., a reference to
``Jane'' can be realized as ``she''), a characteristic
feature of reference in natural languages.
Experiments on three representative
applications show our model variants
outperform models based on deterministic
attention and standard language modeling
baselines.},
	Author = {Yang, Zichao and Blunsom, Phil and Dyer, Chris and Ling, Wang},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Comment = {enable coreferent mentions to have different forms},
	Pages = {1850--1859},
	Title = {Reference-Aware Language Models},
	Year = {2017}}

@inproceedings{Lebret2016Neural,
	Abstract = {This paper introduces a neural model for
concept-to-text generation that scales to large,
rich domains. It generates biographical sentences
from fact tables on a new dataset of
biographies from Wikipedia. This set is an
order of magnitude larger than existing resources
with over 700k samples and a 400k
vocabulary. Our model builds on conditional
neural language models for text generation.
To deal with the large vocabulary, we extend
these models to mix a fixed vocabulary
with copy actions that transfer sample-specific
words from the input database to the generated
output sentence. To deal with structured
data, we allow the model to embed words
differently depending on the data fields in
which they occur. Our neural model significantly
outperforms a Templated Kneser-Ney
language model by nearly 15 BLEU},
	Author = {Lebret, Remi and Grangier, David and Auli, Michael},
	Booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	Comment = {exploits structured data both globally and locally word embedding, table embedding (global + local), aggregating feed-forward neural language model},
	Pages = {1203--1213},
	Title = {Neural Text Generation from Structured Data with Application to the Biography Domain},
	Year = {2016}}

@article{Bao2019Text,
	Author = {J. Bao and D. Tang and N. Duan and Z. Yan and M. Zhou and T. Zhao},
	Date-Modified = {2019-01-22 12:38:08 +0000},
	Issn = {2329-9290},
	Journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	Month = {Feb},
	Number = {2},
	Pages = {311-320},
	Title = {Text Generation From Tables},
	Volume = {27},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TASLP.2018.2878381}}

@inproceedings{Thomaidou2013Automated,
	Abstract = {Products, services or brands can be advertised alongside the search results in major search engines, while recently smaller displays on devices like tablets and smartphones have imposed the need for smaller ad texts. In this paper, we propose a method that produces in an automated manner compact text ads (promotional text snippets), given as input a product description webpage (landing page). The challenge is to produce a small comprehensive ad while maintaining at the same time relevance, clarity, and attractiveness. Our method includes the following phases. Initially, it extracts relevant and important n-grams (keywords) given the landing page. The keywords reserved must have a positive meaning in order to have a call-to-action style, thus we attempt sentiment analysis on them. Next, we build an Advertising Language Model to evaluate phrases in terms of their marketing appeal. We experiment with two variations of our method and we show that they outperform all the baseline approaches.},
	Acmid = {2507876},
	Address = {New York, NY, USA},
	Author = {Thomaidou, Stamatina and Lourentzou, Ismini and Katsivelis-Perakis, Panagiotis and Vazirgiannis, Michalis},
	Booktitle = {Proceedings of the 22nd ACM international conference on Conference on information \&\#38; knowledge management},
	Doi = {10.1145/2505515.2507876},
	Isbn = {978-1-4503-2263-8},
	Keywords = {automated ad-text generation, online advertising, sponsored search, textual advertising},
	Location = {San Francisco, California, USA},
	Numpages = {4},
	Pages = {1841--1844},
	Publisher = {ACM},
	Series = {CIKM '13},
	Title = {Automated snippet generation for online advertising},
	Url = {http://doi.acm.org/10.1145/2505515.2507876},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2505515.2507876},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2505515.2507876}}

@article{Kukich1983Knowledge,
	Abstract = {Knowledge-Based Report Generation is a technique for automatically generating natural language summaries from databases. It is so named because it applies the tools of knowledge-based expert systems design to the problem of text generation. The technique is currently being applied to the design of an automatic natural language stock report generator. Examples drawn from the implementation of the stock report generator are used to describe the components of a knowledge-based report generator.},
	Acmid = {511828},
	Address = {New York, NY, USA},
	Author = {Kukich, Karen},
	Doi = {10.1145/1013230.511828},
	Issn = {0163-5840},
	Issue_Date = {Summer 1983},
	Journal = {SIGIR Forum},
	Keywords = {databases, knowledge-based expert systems, natural language processing, text generation},
	Month = jun,
	Number = {4},
	Numpages = {5},
	Pages = {246--250},
	Publisher = {ACM},
	Title = {Knowledge-Based Report Generation: A Technique for Automatically Generating Natural Language Reports from Databases},
	Url = {http://doi.acm.org/10.1145/1013230.511828},
	Volume = {17},
	Year = {1983},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1013230.511828},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1013230.511828}}

@inproceedings{Sutskever2011Generating,
	Abstract = {Recurrent Neural Networks (RNNs) are very
powerful sequence models that do not enjoy
widespread use because it is extremely diffi-
cult to train them properly. Fortunately, recent
advances in Hessian-free optimization have
been able to overcome the difficulties associated
with training RNNs, making it possible to apply
them successfully to challenging sequence problems.
In this paper we demonstrate the power
of RNNs trained with the new Hessian-Free optimizer
(HF) by applying them to character-level
language modeling tasks. The standard RNN architecture,
while effective, is not ideally suited
for such tasks, so we introduce a new RNN
variant that uses multiplicative (or ``gated'') connections
which allow the current input character
to determine the transition matrix from one
hidden state vector to the next. After training
the multiplicative RNN with the HF optimizer
for five days on 8 high-end Graphics Processing
Units, we were able to surpass the performance
of the best previous single method for characterlevel
language modeling -- a hierarchical nonparametric
sequence model. To our knowledge
this represents the largest recurrent neural network
application to date.},
	Author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
	Booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
	Pages = {1017--1024},
	Title = {Generating text with recurrent neural networks},
	Year = {2011}}

@inproceedings{Li2018Point,
	Abstract = {The task of data-to-text generation aims to generate descriptive texts conditioned on a number
of database records, and recent neural models have shown significant progress on this task. The
attention based encoder-decoder models with copy mechanism have achieved state-of-the-art
results on a few data-to-text datasets. However, such models still face the problem of putting
incorrect data records in the generated texts, especially on some more challenging datasets like
ROTOWIRE. In this paper, we propose a two-stage approach with a delayed copy mechanism
to improve the precision of data records in the generated texts. Our approach first adopts an
encoder-decoder model to generate a template text with data slots to be filled and then leverages
a proposed delayed copy mechanism to fill in the slots with proper data records. Our delayed
copy mechanism can take into account all the information of the input data records and the
full generated template text by using double attention, position-aware attention and a pairwise
ranking loss. The two models in the two stages are trained separately. Evaluation results on the
ROTOWIRE dataset verify the efficacy of our proposed approach to generate better templates and
copy data records more precisely.},
	Author = {Li, Liunian and Wan, Xiaojun},
	Booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	Pages = {1044--1055},
	Title = {Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism},
	Year = {2018}}

@inproceedings{Marcheggiani2018Deep,
	Author = {Marcheggiani, Diego and Perez-Beltrachini, Laura},
	Booktitle = {Proceedings of the 11th International Conference on Natural Language Generation},
	Pages = {1--9},
	Title = {Deep Graph Convolutional Encoders for Structured Data to Text Generation},
	Year = {2018}}

@inproceedings{Karpathy2015Deep,
	Author = {Karpathy, Andrej and Fei-Fei, Li},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {3128--3137},
	Title = {Deep visual-semantic alignments for generating image descriptions},
	Year = {2015}}

@inproceedings{Dehghani2017Neural,
	Address = {New York, NY, USA},
	Author = {Dehghani, Mostafa and Zamani, Hamed and Severyn, Aliaksei and Kamps, Jaap and Croft, W. Bruce},
	Booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	Date-Modified = {2019-01-22 12:41:20 +0000},
	Location = {Shinjuku, Tokyo, Japan},
	Numpages = {10},
	Pages = {65--74},
	Publisher = {ACM},
	Series = {SIGIR '17},
	Title = {Neural Ranking Models with Weak Supervision},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3077136.3080832},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3077136.3080832}}

@article{Zhou2018brief,
	Author = {Zhou, Zhi-Hua},
	Date-Modified = {2019-01-22 13:21:26 +0000},
	Journal = {National Science Review},
	Number = {1},
	Pages = {44-53},
	Title = {A brief introduction to weakly supervised learning},
	Volume = {5},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/nsr/nwx106}}
